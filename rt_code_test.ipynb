{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import models\n",
    "# from timm.models import create_model\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import models\n",
    "from timm.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> load_time: 0:00:01\n",
      "Time to process 16 frames: 310 milliseconds\n",
      "Time to process 16 frames: 59 milliseconds\n",
      "Time to process 16 frames: 56 milliseconds\n",
      "Time to process 16 frames: 58 milliseconds\n",
      "Time to process 16 frames: 58 milliseconds\n",
      "Time to process 16 frames: 59 milliseconds\n",
      "Time to process 16 frames: 58 milliseconds\n",
      "Time to process 16 frames: 42 milliseconds\n",
      "Time to process 16 frames: 57 milliseconds\n",
      "Time to process 16 frames: 60 milliseconds\n",
      "==>> total time: 0:00:07\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.now()\n",
    "\n",
    "time_start_str = time_start.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "segments_num = 1\n",
    "# 모델에 들어갈 frame수는 16 * segments_num\n",
    "\n",
    "backbone = create_model(\n",
    "    \"vit_small_patch16_224\",\n",
    "    img_size=224,\n",
    "    pretrained=False,\n",
    "    num_classes=710,\n",
    "    all_frames=16 * segments_num,\n",
    "    # tubelet_size=args.tubelet_size,\n",
    "    # drop_rate=args.drop,\n",
    "    # drop_path_rate=args.drop_path,\n",
    "    # attn_drop_rate=args.attn_drop_rate,\n",
    "    # head_drop_rate=args.head_drop_rate,\n",
    "    # drop_block_rate=None,\n",
    "    # use_mean_pooling=args.use_mean_pooling,\n",
    "    # init_scale=args.init_scale,\n",
    "    # with_cp=args.with_checkpoint,\n",
    ")\n",
    "\n",
    "load_dict = torch.load(\n",
    "    \"/data/ephemeral/home/level2-3-cv-finalproject-cv-06/pths/vit_s_k710_dl_from_giant.pth\"\n",
    ")\n",
    "# backbone pth 경로\n",
    "\n",
    "backbone.load_state_dict(load_dict[\"module\"])\n",
    "model = nn.Sequential(backbone, nn.Linear(710, 1), nn.Sigmoid())\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "tf = A.Resize(224, 224)\n",
    "\n",
    "model_load_time = datetime.now()\n",
    "load_time = model_load_time - time_start\n",
    "load_time = str(load_time).split(\".\")[0]\n",
    "print(f\"==>> load_time: {load_time}\")\n",
    "# 시웅: 16*8 frame을 받는 모델 로드하는데 주피터 노트북 기준 11.7초\n",
    "# => 모델을 이전에 미리 load 해놓으면 좋을 듯?\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\n",
    "# 시웅: 제 컴퓨터 카메라는 frame 가로세로 변경이 안되서 일단 albumentations Resize를 사용했습니다.\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "frames = []\n",
    "count = 0\n",
    "t_count = 0\n",
    "\n",
    "outputs_score = []\n",
    "output_frames = []\n",
    "\n",
    "while True:\n",
    "    loop_start = datetime.now()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    # frame.shape = (height, width, 3)\n",
    "    if not ret:\n",
    "        print(\"Cam Error\")\n",
    "        break\n",
    "\n",
    "    frame = tf(image=frame)[\"image\"]\n",
    "    # frame.shape = (224, 224, 3)\n",
    "\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    # frame.shape = (1, 224, 224, 3)\n",
    "    frames.append(frame)\n",
    "    count += 1\n",
    "\n",
    "    if count == 16 * segments_num:\n",
    "        assert len(frames) == 16 * segments_num\n",
    "        frames = np.concatenate(frames)\n",
    "        # in_frames.shape = (16 * segments_num, 224, 224, 3)\n",
    "        in_frames = frames.transpose(3, 0, 1, 2)\n",
    "        # # in_frames.shape = (RGB 3, frame T=16 * segments_num, H=224, W=224)\n",
    "        in_frames = np.expand_dims(in_frames, axis=0)\n",
    "        # in_frames.shape = (1, 3, 16 * segments_num, 224, 224)\n",
    "        in_frames = torch.from_numpy(in_frames).float().to(\"cuda\")\n",
    "        # in_frames.shape == torch.Size([1, 3, 16 * segments_num, 224, 224])\n",
    "        with torch.no_grad():\n",
    "            output = model(in_frames)\n",
    "            # output.shape == torch.Size([1, 1])\n",
    "\n",
    "        output = torch.squeeze(output)\n",
    "        output = output.item()\n",
    "        outputs_score.append(output)\n",
    "        if output >= threshold:\n",
    "            output_frames.append(((len(outputs_score) - 1, frames.copy())))\n",
    "        count = 0\n",
    "        frames = []\n",
    "        loop_end = datetime.now()\n",
    "        loop_time = (loop_end - loop_start).total_seconds()\n",
    "        print(f\"Time to process {16*segments_num} frames: {loop_time * 1000:.0f} milliseconds\")\n",
    "        loop_start = loop_end\n",
    "        t_count +=1\n",
    "        if t_count == 10:\n",
    "            break\n",
    "\n",
    "time_end = datetime.now()\n",
    "total_time = time_end - time_start\n",
    "total_time = str(total_time).split(\".\")[0]\n",
    "print(f\"==>> total time: {total_time}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49214890599250793,\n",
       " 0.49214890599250793,\n",
       " 0.49214890599250793,\n",
       " 0.583429753780365,\n",
       " 0.6272579431533813,\n",
       " 0.6254044771194458,\n",
       " 0.6221456527709961,\n",
       " 0.6297355890274048,\n",
       " 0.6246762275695801,\n",
       " 0.6333191394805908]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 224, 224, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_frames[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
